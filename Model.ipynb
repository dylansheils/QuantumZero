{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_frTPqD_LWge",
        "outputId": "561ed393-c8ab-4472-8790-77b8101cc0c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.8/dist-packages (2.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.25.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (5.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bkWnKuzCyXb",
        "outputId": "b6860fef-b2f6-4d06-d461-01cd25cbc0c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: torch-geometric 2.2.0\n",
            "Uninstalling torch-geometric-2.2.0:\n",
            "  Successfully uninstalled torch-geometric-2.2.0\n",
            "\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.0+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.16%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.16+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_cluster-1.6.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-cluster) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-cluster) (1.22.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-t8h6n1ky\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-t8h6n1ky\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit eac6cf1f487ccf5eb11fe52b0c7cfcd0c15aad38\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.3.0) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.3.0) (1.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.3.0) (2.11.3)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.3.0) (5.9.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.3.0) (3.0.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.3.0) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.3.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.3.0) (2.25.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric==2.3.0) (2.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric==2.3.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric==2.3.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric==2.3.0) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric==2.3.0) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric==2.3.0) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.0-py3-none-any.whl size=871496 sha256=2f90dab2396059a9e3187f407719e061e7a20d2f7ac0cc6c3581ad2e2febb780\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t_cve8pa/wheels/ba/e1/8e/28297c3201c884d3ea8c47ba71a9e71e547e556c0caa9cf5a2\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtYMuTi7LVTE",
        "outputId": "8ff3520a-1135-4449-c791-d9a6277446a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W5ifN-y7LVTJ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BPP_0APyLVTK"
      },
      "outputs": [],
      "source": [
        "def visualize_graph(G, color):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
        "                     node_color=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KoAJ9-LYLVTL"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import KarateClub\n",
        "\n",
        "dataset = KarateClub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BAZbRiz0LVTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "617f417e-e917-448b-c5ad-789c7f0b402e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset properties\n",
            "==============================================================\n",
            "Dataset: KarateClub()\n",
            "Number of graphs in the dataset: 1\n",
            "Number of features: 34\n",
            "Number of classes: 4\n",
            "Graph properties\n",
            "==============================================================\n",
            "Number of nodes: 34\n",
            "Number of edges: 156\n",
            "Average node degree: 4.59\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ],
      "source": [
        "print('Dataset properties')\n",
        "print('==============================================================')\n",
        "print(f'Dataset: {dataset}') #This prints the name of the dataset\n",
        "print(f'Number of graphs in the dataset: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}') #Number of features each node in the dataset has\n",
        "print(f'Number of classes: {dataset.num_classes}') #Number of classes that a node can be classified into\n",
        "\n",
        "\n",
        "#Since we have one graph in the dataset, we will select the graph and explore it's properties\n",
        "\n",
        "data = dataset[0]\n",
        "print('Graph properties')\n",
        "print('==============================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}') #Number of nodes in the graph\n",
        "print(f'Number of edges: {data.num_edges}') #Number of edges in the graph\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}') # Average number of nodes in the graph\n",
        "print(f'Contains isolated nodes: {data.has_isolated_nodes()}') #Does the graph contains nodes that are not connected\n",
        "print(f'Contains self-loops: {data.has_self_loops()}') #Does the graph contains nodes that are linked to themselves\n",
        "print(f'Is undirected: {data.is_undirected()}') #Is the graph an undirected graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3PgEIhOLVTM"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "visualize_graph(G, color=data.y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "_Hb6xBWqLVTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead0193c-1487-434a-a352-73522e0cf2d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (i2h): Linear(in_features=38, out_features=4, bias=True)\n",
            "  (i2o): Linear(in_features=38, out_features=4, bias=True)\n",
            "  (conv): TransformerConv(38, 4, heads=1)\n",
            "  (extractor): Linear(in_features=4, out_features=8, bias=True)\n",
            "  (mixer): Linear(in_features=8, out_features=38, bias=True)\n",
            "  (classifier): Linear(in_features=4, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.nn import Linear, GELU\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn.conv.transformer_conv import TransformerConv\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.hidden_size = 4 \n",
        "        self.i2h = Linear(dataset.num_features + self.hidden_size, self.hidden_size)\n",
        "        self.i2o = Linear(dataset.num_features + self.hidden_size, 4)\n",
        "        self.conv = TransformerConv(self.hidden_size + dataset.num_features, 4)\n",
        "        self.extractor = Linear(4, 4 + self.hidden_size)\n",
        "        self.mixer = Linear(4 + self.hidden_size, dataset.num_features + self.hidden_size)\n",
        "        self.classifier = Linear(4, dataset.num_classes)\n",
        "    def forward(self, input, hidden, edge_index):\n",
        "        x = torch.cat((input, hidden), 1)\n",
        "        GELUa = nn.GELU()\n",
        "        h = self.conv(x, edge_index)\n",
        "        h = GELUa(h)\n",
        "        h = self.extractor(h)\n",
        "        h = self.mixer(h)\n",
        "        nextHidden = self.i2h(h)\n",
        "        outputHidden = self.i2o(h)\n",
        "        out = self.classifier(outputHidden)\n",
        "        return out, nextHidden\n",
        "    def initHidden(self):\n",
        "        return Variable(torch.zeros(1, self.hidden_size))\n",
        "\n",
        "model = GCN()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "t_nkgUC3LVTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14a21e1-d742-4c5c-c95b-d9c2e0b2908d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.4315178394317627\n",
            "Epoch: 1, Loss: 1.4095698595046997\n",
            "Epoch: 2, Loss: 1.3964595794677734\n",
            "Epoch: 3, Loss: 1.3876399993896484\n",
            "Epoch: 4, Loss: 1.3820247650146484\n",
            "Epoch: 5, Loss: 1.3792479038238525\n",
            "Epoch: 6, Loss: 1.3770573139190674\n",
            "Epoch: 7, Loss: 1.3723692893981934\n",
            "Epoch: 8, Loss: 1.3639017343521118\n",
            "Epoch: 9, Loss: 1.351718544960022\n",
            "Epoch: 10, Loss: 1.3359968662261963\n",
            "Epoch: 11, Loss: 1.3163366317749023\n",
            "Epoch: 12, Loss: 1.2915457487106323\n",
            "Epoch: 13, Loss: 1.2599763870239258\n",
            "Epoch: 14, Loss: 1.220171570777893\n",
            "Epoch: 15, Loss: 1.171316146850586\n",
            "Epoch: 16, Loss: 1.1136327981948853\n",
            "Epoch: 17, Loss: 1.0488715171813965\n",
            "Epoch: 18, Loss: 0.9804044365882874\n",
            "Epoch: 19, Loss: 0.9126631617546082\n",
            "Epoch: 20, Loss: 0.849560558795929\n",
            "Epoch: 21, Loss: 0.7926795482635498\n",
            "Epoch: 22, Loss: 0.7405542135238647\n",
            "Epoch: 23, Loss: 0.6901300549507141\n",
            "Epoch: 24, Loss: 0.6420490741729736\n",
            "Epoch: 25, Loss: 0.6040982604026794\n",
            "Epoch: 26, Loss: 0.5735111236572266\n",
            "Epoch: 27, Loss: 0.5345556139945984\n",
            "Epoch: 28, Loss: 0.49520379304885864\n",
            "Epoch: 29, Loss: 0.4640229642391205\n",
            "Epoch: 30, Loss: 0.432340145111084\n",
            "Epoch: 31, Loss: 0.39463722705841064\n",
            "Epoch: 32, Loss: 0.3531953692436218\n",
            "Epoch: 33, Loss: 0.3088787794113159\n",
            "Epoch: 34, Loss: 0.26219454407691956\n",
            "Epoch: 35, Loss: 0.21324461698532104\n",
            "Epoch: 36, Loss: 0.1659197211265564\n",
            "Epoch: 37, Loss: 0.13441911339759827\n",
            "Epoch: 38, Loss: 0.1295325756072998\n",
            "Epoch: 39, Loss: 0.09663056582212448\n",
            "Epoch: 40, Loss: 0.057461973279714584\n",
            "Epoch: 41, Loss: 0.04000452533364296\n",
            "Epoch: 42, Loss: 0.0332116074860096\n",
            "Epoch: 43, Loss: 0.02759067714214325\n",
            "Epoch: 44, Loss: 0.020362818613648415\n",
            "Epoch: 45, Loss: 0.013394270092248917\n",
            "Epoch: 46, Loss: 0.008805163204669952\n",
            "Epoch: 47, Loss: 0.00658944807946682\n",
            "Epoch: 48, Loss: 0.005770173389464617\n",
            "Epoch: 49, Loss: 0.005487044341862202\n",
            "Epoch: 50, Loss: 0.005212068557739258\n",
            "Epoch: 51, Loss: 0.004718518350273371\n",
            "Epoch: 52, Loss: 0.00401730602607131\n",
            "Epoch: 53, Loss: 0.0032441341318190098\n",
            "Epoch: 54, Loss: 0.00253609218634665\n",
            "Epoch: 55, Loss: 0.0019681970588862896\n",
            "Epoch: 56, Loss: 0.0015543195186182857\n",
            "Epoch: 57, Loss: 0.0012741264654323459\n",
            "Epoch: 58, Loss: 0.001095174578949809\n",
            "Epoch: 59, Loss: 0.0009847009787335992\n",
            "Epoch: 60, Loss: 0.0009137344313785434\n",
            "Epoch: 61, Loss: 0.0008592490339651704\n",
            "Epoch: 62, Loss: 0.0008051931508816779\n",
            "Epoch: 63, Loss: 0.0007433864520862699\n",
            "Epoch: 64, Loss: 0.0006727493600919843\n",
            "Epoch: 65, Loss: 0.0005968702025711536\n",
            "Epoch: 66, Loss: 0.00052115588914603\n",
            "Epoch: 67, Loss: 0.0004504543321672827\n",
            "Epoch: 68, Loss: 0.00038783345371484756\n",
            "Epoch: 69, Loss: 0.0003344883443787694\n",
            "Epoch: 70, Loss: 0.00029033442842774093\n",
            "Epoch: 71, Loss: 0.00025445257779210806\n",
            "Epoch: 72, Loss: 0.00022559499484486878\n",
            "Epoch: 73, Loss: 0.0002024830027949065\n",
            "Epoch: 74, Loss: 0.00018395634833723307\n",
            "Epoch: 75, Loss: 0.0001690328645054251\n",
            "Epoch: 76, Loss: 0.00015687903214711696\n",
            "Epoch: 77, Loss: 0.00014686959912069142\n",
            "Epoch: 78, Loss: 0.0001386175281368196\n",
            "Epoch: 79, Loss: 0.00013158668298274279\n",
            "Epoch: 80, Loss: 0.00012559840979520231\n",
            "Epoch: 81, Loss: 0.00012038463319186121\n",
            "Epoch: 82, Loss: 0.00011576663382584229\n",
            "Epoch: 83, Loss: 0.00011171465303050354\n",
            "Epoch: 84, Loss: 0.00010796055721584707\n",
            "Epoch: 85, Loss: 0.00010462355567142367\n",
            "Epoch: 86, Loss: 0.00010146528074983507\n",
            "Epoch: 87, Loss: 9.854532981989905e-05\n",
            "Epoch: 88, Loss: 9.589352703187615e-05\n",
            "Epoch: 89, Loss: 9.330129250884056e-05\n",
            "Epoch: 90, Loss: 9.091760875890031e-05\n",
            "Epoch: 91, Loss: 8.865309064276516e-05\n",
            "Epoch: 92, Loss: 8.647795038996264e-05\n",
            "Epoch: 93, Loss: 8.439218072453514e-05\n",
            "Epoch: 94, Loss: 8.245539356721565e-05\n",
            "Epoch: 95, Loss: 8.06079842732288e-05\n",
            "Epoch: 96, Loss: 7.887975516496226e-05\n",
            "Epoch: 97, Loss: 7.71813138271682e-05\n",
            "Epoch: 98, Loss: 7.557225762866437e-05\n",
            "Epoch: 99, Loss: 7.40227842470631e-05\n",
            "Epoch: 100, Loss: 7.256270328070968e-05\n",
            "Epoch: 101, Loss: 7.122180250007659e-05\n",
            "Epoch: 102, Loss: 6.988090899540111e-05\n",
            "Epoch: 103, Loss: 6.865919567644596e-05\n",
            "Epoch: 104, Loss: 6.74374750815332e-05\n",
            "Epoch: 105, Loss: 6.627534457948059e-05\n",
            "Epoch: 106, Loss: 6.526221841340885e-05\n",
            "Epoch: 107, Loss: 6.421927537303418e-05\n",
            "Epoch: 108, Loss: 6.329553434625268e-05\n",
            "Epoch: 109, Loss: 6.228239362826571e-05\n",
            "Epoch: 110, Loss: 6.141823541838676e-05\n",
            "Epoch: 111, Loss: 6.05838795308955e-05\n",
            "Epoch: 112, Loss: 5.9779318689834327e-05\n",
            "Epoch: 113, Loss: 5.900456017116085e-05\n",
            "Epoch: 114, Loss: 5.822979437652975e-05\n",
            "Epoch: 115, Loss: 5.7514625950716436e-05\n",
            "Epoch: 116, Loss: 5.6859054893720895e-05\n",
            "Epoch: 117, Loss: 5.6173685152316466e-05\n",
            "Epoch: 118, Loss: 5.5547909141751006e-05\n",
            "Epoch: 119, Loss: 5.4922136769164354e-05\n",
            "Epoch: 120, Loss: 5.438576408778317e-05\n",
            "Epoch: 121, Loss: 5.381958180805668e-05\n",
            "Epoch: 122, Loss: 5.3253406804287806e-05\n",
            "Epoch: 123, Loss: 5.271702684694901e-05\n",
            "Epoch: 124, Loss: 5.224024425842799e-05\n",
            "Epoch: 125, Loss: 5.173366662347689e-05\n",
            "Epoch: 126, Loss: 5.122708171256818e-05\n",
            "Epoch: 127, Loss: 5.075030276202597e-05\n",
            "Epoch: 128, Loss: 5.0273512897547334e-05\n",
            "Epoch: 129, Loss: 4.982653626939282e-05\n",
            "Epoch: 130, Loss: 4.940934741171077e-05\n",
            "Epoch: 131, Loss: 4.896236350759864e-05\n",
            "Epoch: 132, Loss: 4.854517464991659e-05\n",
            "Epoch: 133, Loss: 4.815779175260104e-05\n",
            "Epoch: 134, Loss: 4.774060289491899e-05\n",
            "Epoch: 135, Loss: 4.732342131319456e-05\n",
            "Epoch: 136, Loss: 4.6936034777900204e-05\n",
            "Epoch: 137, Loss: 4.654864824260585e-05\n",
            "Epoch: 138, Loss: 4.616125806933269e-05\n",
            "Epoch: 139, Loss: 4.5773871534038335e-05\n",
            "Epoch: 140, Loss: 4.5446086005540565e-05\n",
            "Epoch: 141, Loss: 4.50884981546551e-05\n",
            "Epoch: 142, Loss: 4.470110798138194e-05\n",
            "Epoch: 143, Loss: 4.437331517692655e-05\n",
            "Epoch: 144, Loss: 4.4045526010449976e-05\n",
            "Epoch: 145, Loss: 4.3717740481952205e-05\n",
            "Epoch: 146, Loss: 4.3360145355109125e-05\n",
            "Epoch: 147, Loss: 4.300255750422366e-05\n",
            "Epoch: 148, Loss: 4.2674764699768275e-05\n",
            "Epoch: 149, Loss: 4.2376774217700586e-05\n",
            "Epoch: 150, Loss: 4.20489814132452e-05\n",
            "Epoch: 151, Loss: 4.175099456915632e-05\n",
            "Epoch: 152, Loss: 4.1452996811131015e-05\n",
            "Epoch: 153, Loss: 4.112520764465444e-05\n",
            "Epoch: 154, Loss: 4.082721352460794e-05\n",
            "Epoch: 155, Loss: 4.052922304254025e-05\n",
            "Epoch: 156, Loss: 4.029082992929034e-05\n",
            "Epoch: 157, Loss: 3.9963037124834955e-05\n",
            "Epoch: 158, Loss: 3.966504300478846e-05\n",
            "Epoch: 159, Loss: 3.939684393117204e-05\n",
            "Epoch: 160, Loss: 3.915845445590094e-05\n",
            "Epoch: 161, Loss: 3.886046033585444e-05\n",
            "Epoch: 162, Loss: 3.856246257782914e-05\n",
            "Epoch: 163, Loss: 3.835387178696692e-05\n",
            "Epoch: 164, Loss: 3.808567635132931e-05\n",
            "Epoch: 165, Loss: 3.775787627091631e-05\n",
            "Epoch: 166, Loss: 3.75194831576664e-05\n",
            "Epoch: 167, Loss: 3.7310888728825375e-05\n",
            "Epoch: 168, Loss: 3.704268965520896e-05\n",
            "Epoch: 169, Loss: 3.6744695535162464e-05\n",
            "Epoch: 170, Loss: 3.653609746834263e-05\n",
            "Epoch: 171, Loss: 3.629770435509272e-05\n",
            "Epoch: 172, Loss: 3.605930396588519e-05\n",
            "Epoch: 173, Loss: 3.582091085263528e-05\n",
            "Epoch: 174, Loss: 3.561231278581545e-05\n",
            "Epoch: 175, Loss: 3.534411371219903e-05\n",
            "Epoch: 176, Loss: 3.513551928335801e-05\n",
            "Epoch: 177, Loss: 3.489711889415048e-05\n",
            "Epoch: 178, Loss: 3.4718323149718344e-05\n",
            "Epoch: 179, Loss: 3.450972508289851e-05\n",
            "Epoch: 180, Loss: 3.42415260092821e-05\n",
            "Epoch: 181, Loss: 3.406273026484996e-05\n",
            "Epoch: 182, Loss: 3.385412856005132e-05\n",
            "Epoch: 183, Loss: 3.36455341312103e-05\n",
            "Epoch: 184, Loss: 3.343693242641166e-05\n",
            "Epoch: 185, Loss: 3.3258133044000715e-05\n",
            "Epoch: 186, Loss: 3.3019736292771995e-05\n",
            "Epoch: 187, Loss: 3.2811134587973356e-05\n",
            "Epoch: 188, Loss: 3.2602536521153525e-05\n",
            "Epoch: 189, Loss: 3.242374077672139e-05\n",
            "Epoch: 190, Loss: 3.224493775633164e-05\n",
            "Epoch: 191, Loss: 3.2066138373920694e-05\n",
            "Epoch: 192, Loss: 3.188734262948856e-05\n",
            "Epoch: 193, Loss: 3.167874092468992e-05\n",
            "Epoch: 194, Loss: 3.155954254907556e-05\n",
            "Epoch: 195, Loss: 3.1321138521889225e-05\n",
            "Epoch: 196, Loss: 3.120194378425367e-05\n",
            "Epoch: 197, Loss: 3.102314076386392e-05\n",
            "Epoch: 198, Loss: 3.084434138145298e-05\n",
            "Epoch: 199, Loss: 3.0635743314633146e-05\n",
            "Epoch: 200, Loss: 3.048674261663109e-05\n",
            "Epoch: 201, Loss: 3.033774373761844e-05\n",
            "Epoch: 202, Loss: 3.0158942536218092e-05\n",
            "Epoch: 203, Loss: 3.0039744160603732e-05\n",
            "Epoch: 204, Loss: 2.983114063681569e-05\n",
            "Epoch: 205, Loss: 2.9711940442211926e-05\n",
            "Epoch: 206, Loss: 2.9533141059800982e-05\n",
            "Epoch: 207, Loss: 2.9354338039411232e-05\n",
            "Epoch: 208, Loss: 2.9264940167195164e-05\n",
            "Epoch: 209, Loss: 2.9086138965794817e-05\n",
            "Epoch: 210, Loss: 2.893713826779276e-05\n",
            "Epoch: 211, Loss: 2.8788137569790706e-05\n",
            "Epoch: 212, Loss: 2.863913687178865e-05\n",
            "Epoch: 213, Loss: 2.8490136173786595e-05\n",
            "Epoch: 214, Loss: 2.8370937798172235e-05\n",
            "Epoch: 215, Loss: 2.8192134777782485e-05\n",
            "Epoch: 216, Loss: 2.8102735086577013e-05\n",
            "Epoch: 217, Loss: 2.7953732569585554e-05\n",
            "Epoch: 218, Loss: 2.7774929549195804e-05\n",
            "Epoch: 219, Loss: 2.7655731173581444e-05\n",
            "Epoch: 220, Loss: 2.756632966338657e-05\n",
            "Epoch: 221, Loss: 2.7417328965384513e-05\n",
            "Epoch: 222, Loss: 2.7268328267382458e-05\n",
            "Epoch: 223, Loss: 2.7149128072778694e-05\n",
            "Epoch: 224, Loss: 2.7029924240196124e-05\n",
            "Epoch: 225, Loss: 2.6940524548990652e-05\n",
            "Epoch: 226, Loss: 2.6791523850988597e-05\n",
            "Epoch: 227, Loss: 2.6702124159783125e-05\n",
            "Epoch: 228, Loss: 2.6553119823802263e-05\n",
            "Epoch: 229, Loss: 2.646372013259679e-05\n",
            "Epoch: 230, Loss: 2.6314719434594736e-05\n",
            "Epoch: 231, Loss: 2.6225316105410457e-05\n",
            "Epoch: 232, Loss: 2.6106115910806693e-05\n",
            "Epoch: 233, Loss: 2.5957113393815234e-05\n",
            "Epoch: 234, Loss: 2.589751238701865e-05\n",
            "Epoch: 235, Loss: 2.5748511689016595e-05\n",
            "Epoch: 236, Loss: 2.565911017882172e-05\n",
            "Epoch: 237, Loss: 2.5539909984217957e-05\n",
            "Epoch: 238, Loss: 2.5450506655033678e-05\n",
            "Epoch: 239, Loss: 2.5361106963828206e-05\n",
            "Epoch: 240, Loss: 2.524190495023504e-05\n",
            "Epoch: 241, Loss: 2.5152503440040164e-05\n",
            "Epoch: 242, Loss: 2.5033301426446997e-05\n",
            "Epoch: 243, Loss: 2.4914097593864426e-05\n",
            "Epoch: 244, Loss: 2.4854496587067842e-05\n",
            "Epoch: 245, Loss: 2.4735298211453483e-05\n",
            "Epoch: 246, Loss: 2.461609255988151e-05\n",
            "Epoch: 247, Loss: 2.455649337207433e-05\n",
            "Epoch: 248, Loss: 2.4437293177470565e-05\n",
            "Epoch: 249, Loss: 2.4347889848286286e-05\n",
            "Epoch: 250, Loss: 2.4258486519102007e-05\n",
            "Epoch: 251, Loss: 2.413928450550884e-05\n",
            "Epoch: 252, Loss: 2.4109487640089355e-05\n",
            "Epoch: 253, Loss: 2.3990283807506785e-05\n",
            "Epoch: 254, Loss: 2.3900884116301313e-05\n",
            "Epoch: 255, Loss: 2.3811480787117034e-05\n",
            "Epoch: 256, Loss: 2.3692276954534464e-05\n",
            "Epoch: 257, Loss: 2.3662476451136172e-05\n",
            "Epoch: 258, Loss: 2.3573074940941297e-05\n",
            "Epoch: 259, Loss: 2.3483675249735825e-05\n",
            "Epoch: 260, Loss: 2.3364471417153254e-05\n",
            "Epoch: 261, Loss: 2.3275068087968975e-05\n",
            "Epoch: 262, Loss: 2.3215467081172392e-05\n",
            "Epoch: 263, Loss: 2.3155867893365212e-05\n",
            "Epoch: 264, Loss: 2.303666406078264e-05\n",
            "Epoch: 265, Loss: 2.2977063053986058e-05\n",
            "Epoch: 266, Loss: 2.2887661543791182e-05\n",
            "Epoch: 267, Loss: 2.2798260033596307e-05\n",
            "Epoch: 268, Loss: 2.2738659026799724e-05\n",
            "Epoch: 269, Loss: 2.267905802000314e-05\n",
            "Epoch: 270, Loss: 2.258965469081886e-05\n",
            "Epoch: 271, Loss: 2.2500251361634582e-05\n",
            "Epoch: 272, Loss: 2.2470452677225694e-05\n",
            "Epoch: 273, Loss: 2.2381049348041415e-05\n",
            "Epoch: 274, Loss: 2.232144834124483e-05\n",
            "Epoch: 275, Loss: 2.2232045012060553e-05\n",
            "Epoch: 276, Loss: 2.217244400526397e-05\n",
            "Epoch: 277, Loss: 2.2053241991670802e-05\n",
            "Epoch: 278, Loss: 2.199364098487422e-05\n",
            "Epoch: 279, Loss: 2.1934039978077635e-05\n",
            "Epoch: 280, Loss: 2.187443897128105e-05\n",
            "Epoch: 281, Loss: 2.178503382310737e-05\n",
            "Epoch: 282, Loss: 2.1725430997321382e-05\n",
            "Epoch: 283, Loss: 2.16658299905248e-05\n",
            "Epoch: 284, Loss: 2.1606228983728215e-05\n",
            "Epoch: 285, Loss: 2.151682747353334e-05\n",
            "Epoch: 286, Loss: 2.1457224647747353e-05\n",
            "Epoch: 287, Loss: 2.133801899617538e-05\n",
            "Epoch: 288, Loss: 2.130822031176649e-05\n",
            "Epoch: 289, Loss: 2.1248619304969907e-05\n",
            "Epoch: 290, Loss: 2.118901647918392e-05\n",
            "Epoch: 291, Loss: 2.1129415472387336e-05\n",
            "Epoch: 292, Loss: 2.104001396219246e-05\n",
            "Epoch: 293, Loss: 2.0980411136406474e-05\n",
            "Epoch: 294, Loss: 2.0920808310620487e-05\n",
            "Epoch: 295, Loss: 2.0861207303823903e-05\n",
            "Epoch: 296, Loss: 2.0801604478037916e-05\n",
            "Epoch: 297, Loss: 2.0742003471241333e-05\n",
            "Epoch: 298, Loss: 2.0682400645455346e-05\n",
            "Epoch: 299, Loss: 2.0622799638658762e-05\n",
            "Epoch: 300, Loss: 2.0533398128463887e-05\n",
            "Epoch: 301, Loss: 2.04737953026779e-05\n",
            "Epoch: 302, Loss: 2.0443994799279608e-05\n",
            "Epoch: 303, Loss: 2.0384393792483024e-05\n",
            "Epoch: 304, Loss: 2.0294988644309342e-05\n",
            "Epoch: 305, Loss: 2.023538763751276e-05\n",
            "Epoch: 306, Loss: 2.0205585315125063e-05\n",
            "Epoch: 307, Loss: 2.0116181985940784e-05\n",
            "Epoch: 308, Loss: 2.0086383301531896e-05\n",
            "Epoch: 309, Loss: 1.9996979972347617e-05\n",
            "Epoch: 310, Loss: 1.9996979972347617e-05\n",
            "Epoch: 311, Loss: 1.9907576643163338e-05\n",
            "Epoch: 312, Loss: 1.9847975636366755e-05\n",
            "Epoch: 313, Loss: 1.9788372810580768e-05\n",
            "Epoch: 314, Loss: 1.9758570488193072e-05\n",
            "Epoch: 315, Loss: 1.9669167159008794e-05\n",
            "Epoch: 316, Loss: 1.9579763829824515e-05\n",
            "Epoch: 317, Loss: 1.9579763829824515e-05\n",
            "Epoch: 318, Loss: 1.9520161004038528e-05\n",
            "Epoch: 319, Loss: 1.943075767485425e-05\n",
            "Epoch: 320, Loss: 1.9371156668057665e-05\n",
            "Epoch: 321, Loss: 1.9371156668057665e-05\n",
            "Epoch: 322, Loss: 1.9281753338873386e-05\n",
            "Epoch: 323, Loss: 1.9192350009689108e-05\n",
            "Epoch: 324, Loss: 1.9222152332076803e-05\n",
            "Epoch: 325, Loss: 1.9132749002892524e-05\n",
            "Epoch: 326, Loss: 1.910294668050483e-05\n",
            "Epoch: 327, Loss: 1.904334385471884e-05\n",
            "Epoch: 328, Loss: 1.8983742847922258e-05\n",
            "Epoch: 329, Loss: 1.8953942344523966e-05\n",
            "Epoch: 330, Loss: 1.889433951873798e-05\n",
            "Epoch: 331, Loss: 1.8804934370564297e-05\n",
            "Epoch: 332, Loss: 1.8804934370564297e-05\n",
            "Epoch: 333, Loss: 1.8715531041380018e-05\n",
            "Epoch: 334, Loss: 1.8655930034583434e-05\n",
            "Epoch: 335, Loss: 1.8655930034583434e-05\n",
            "Epoch: 336, Loss: 1.8566526705399156e-05\n",
            "Epoch: 337, Loss: 1.853672438301146e-05\n",
            "Epoch: 338, Loss: 1.8506925698602572e-05\n",
            "Epoch: 339, Loss: 1.8387718228041194e-05\n",
            "Epoch: 340, Loss: 1.8387718228041194e-05\n",
            "Epoch: 341, Loss: 1.8298314898856916e-05\n",
            "Epoch: 342, Loss: 1.823871207307093e-05\n",
            "Epoch: 343, Loss: 1.8208909750683233e-05\n",
            "Epoch: 344, Loss: 1.817910924728494e-05\n",
            "Epoch: 345, Loss: 1.814930874388665e-05\n",
            "Epoch: 346, Loss: 1.8089705918100663e-05\n",
            "Epoch: 347, Loss: 1.8030103092314675e-05\n",
            "Epoch: 348, Loss: 1.8000302588916384e-05\n",
            "Epoch: 349, Loss: 1.7940699763130397e-05\n",
            "Epoch: 350, Loss: 1.7910899259732105e-05\n",
            "Epoch: 351, Loss: 1.7851296433946118e-05\n",
            "Epoch: 352, Loss: 1.7761891285772435e-05\n",
            "Epoch: 353, Loss: 1.7761891285772435e-05\n",
            "Epoch: 354, Loss: 1.7702288459986448e-05\n",
            "Epoch: 355, Loss: 1.7672487956588157e-05\n",
            "Epoch: 356, Loss: 1.761288513080217e-05\n",
            "Epoch: 357, Loss: 1.758308644639328e-05\n",
            "Epoch: 358, Loss: 1.7553284124005586e-05\n",
            "Epoch: 359, Loss: 1.74936812982196e-05\n",
            "Epoch: 360, Loss: 1.7434078472433612e-05\n",
            "Epoch: 361, Loss: 1.7374475646647625e-05\n",
            "Epoch: 362, Loss: 1.7314872820861638e-05\n",
            "Epoch: 363, Loss: 1.7285070498473942e-05\n",
            "Epoch: 364, Loss: 1.722546949167736e-05\n",
            "Epoch: 365, Loss: 1.722546949167736e-05\n",
            "Epoch: 366, Loss: 1.7136064343503676e-05\n",
            "Epoch: 367, Loss: 1.7136064343503676e-05\n",
            "Epoch: 368, Loss: 1.707646151771769e-05\n",
            "Epoch: 369, Loss: 1.7046661014319398e-05\n",
            "Epoch: 370, Loss: 1.7016860510921106e-05\n",
            "Epoch: 371, Loss: 1.698705818853341e-05\n",
            "Epoch: 372, Loss: 1.695725768513512e-05\n",
            "Epoch: 373, Loss: 1.680824971117545e-05\n",
            "Epoch: 374, Loss: 1.680824971117545e-05\n",
            "Epoch: 375, Loss: 1.6748646885389462e-05\n",
            "Epoch: 376, Loss: 1.671884638199117e-05\n",
            "Epoch: 377, Loss: 1.6659243556205183e-05\n",
            "Epoch: 378, Loss: 1.6659243556205183e-05\n",
            "Epoch: 379, Loss: 1.662944305280689e-05\n",
            "Epoch: 380, Loss: 1.6569840227020904e-05\n",
            "Epoch: 381, Loss: 1.654003790463321e-05\n",
            "Epoch: 382, Loss: 1.6510237401234917e-05\n",
            "Epoch: 383, Loss: 1.6480436897836626e-05\n",
            "Epoch: 384, Loss: 1.645063457544893e-05\n",
            "Epoch: 385, Loss: 1.6331427104887553e-05\n",
            "Epoch: 386, Loss: 1.6331427104887553e-05\n",
            "Epoch: 387, Loss: 1.627182609809097e-05\n",
            "Epoch: 388, Loss: 1.6212223272304982e-05\n",
            "Epoch: 389, Loss: 1.6212223272304982e-05\n",
            "Epoch: 390, Loss: 1.6152620446518995e-05\n",
            "Epoch: 391, Loss: 1.6152620446518995e-05\n",
            "Epoch: 392, Loss: 1.6063217117334716e-05\n",
            "Epoch: 393, Loss: 1.6063217117334716e-05\n",
            "Epoch: 394, Loss: 1.603341479494702e-05\n",
            "Epoch: 395, Loss: 1.5973811969161034e-05\n",
            "Epoch: 396, Loss: 1.5914209143375047e-05\n",
            "Epoch: 397, Loss: 1.5884408639976755e-05\n",
            "Epoch: 398, Loss: 1.585460631758906e-05\n",
            "Epoch: 399, Loss: 1.5824803995201364e-05\n",
            "Epoch: 400, Loss: 1.5735400666017085e-05\n"
          ]
        }
      ],
      "source": [
        "model = GCN()\n",
        "criterion = torch.nn.CrossEntropyLoss()  #Initialize the CrossEntropyLoss function.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Initialize the Adam optimizer.\n",
        "\n",
        "def train(data):\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    a = torch.zeros(dataset.num_features, 4)\n",
        "    out, h = model(data.x, a, data.edge_index)  # Perform a single forward pass.\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "    return loss, h\n",
        "\n",
        "for epoch in range(401):\n",
        "    loss, h = train(data)\n",
        "    print(f'Epoch: {epoch}, Loss: {loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "1uFYWXTqLVTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af33402e-8018-4f6e-8af8-b6488691d583"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zvYd2piLVTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTKRXzTmLVTQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "9536ed1e68c952d6cb34c025eefe5d09b0b8df5072189e94cdd6592502baa0d7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.16 64-bit ('MLProject': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
